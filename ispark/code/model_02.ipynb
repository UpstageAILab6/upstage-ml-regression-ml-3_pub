{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/pre_train.csv\")\n",
    "test = pd.read_csv(\"../data/pre_test.csv\")\n",
    "y = pd.read_csv(\"../data/target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train[train.columns] = scaler.fit_transform(train)\n",
    "test[test.columns] = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values.ravel()\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893296, 88) (893296,) (223325, 88) (223325,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMRegressor(n_estimators=10000,\n",
    "                          objective='regression',\n",
    "                          max_depth=-1,\n",
    "                          learning_rate=0.1,\n",
    "                          subsample=0.8,\n",
    "                          colsample_bytree=0.8,\n",
    "                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> training's rmse: 3520.41   \n",
    "> valid_1's rmse: 5888.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5653\n",
      "[LightGBM] [Info] Number of data points in the train set: 893296, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score 10.753211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.159947\ttraining's l2: 0.0255831\tvalid_1's rmse: 0.160393\tvalid_1's l2: 0.025726\n",
      "[200]\ttraining's rmse: 0.137393\ttraining's l2: 0.0188768\tvalid_1's rmse: 0.138424\tvalid_1's l2: 0.0191611\n",
      "[300]\ttraining's rmse: 0.12651\ttraining's l2: 0.0160048\tvalid_1's rmse: 0.127924\tvalid_1's l2: 0.0163645\n",
      "[400]\ttraining's rmse: 0.118858\ttraining's l2: 0.0141271\tvalid_1's rmse: 0.120541\tvalid_1's l2: 0.0145301\n",
      "[500]\ttraining's rmse: 0.112441\ttraining's l2: 0.0126429\tvalid_1's rmse: 0.114408\tvalid_1's l2: 0.0130893\n",
      "[600]\ttraining's rmse: 0.10767\ttraining's l2: 0.0115928\tvalid_1's rmse: 0.109922\tvalid_1's l2: 0.0120829\n",
      "[700]\ttraining's rmse: 0.104214\ttraining's l2: 0.0108605\tvalid_1's rmse: 0.106677\tvalid_1's l2: 0.01138\n",
      "[800]\ttraining's rmse: 0.101392\ttraining's l2: 0.0102804\tvalid_1's rmse: 0.104045\tvalid_1's l2: 0.0108255\n",
      "[900]\ttraining's rmse: 0.09883\ttraining's l2: 0.00976736\tvalid_1's rmse: 0.101698\tvalid_1's l2: 0.0103424\n",
      "[1000]\ttraining's rmse: 0.0966709\ttraining's l2: 0.00934526\tvalid_1's rmse: 0.0998005\tvalid_1's l2: 0.00996014\n",
      "[1100]\ttraining's rmse: 0.0946427\ttraining's l2: 0.00895724\tvalid_1's rmse: 0.098022\tvalid_1's l2: 0.0096083\n",
      "[1200]\ttraining's rmse: 0.0928428\ttraining's l2: 0.00861978\tvalid_1's rmse: 0.0964808\tvalid_1's l2: 0.00930855\n",
      "[1300]\ttraining's rmse: 0.0912176\ttraining's l2: 0.00832066\tvalid_1's rmse: 0.095061\tvalid_1's l2: 0.0090366\n",
      "[1400]\ttraining's rmse: 0.0897082\ttraining's l2: 0.00804756\tvalid_1's rmse: 0.0937656\tvalid_1's l2: 0.00879199\n",
      "[1500]\ttraining's rmse: 0.0883668\ttraining's l2: 0.00780868\tvalid_1's rmse: 0.0926491\tvalid_1's l2: 0.00858386\n",
      "[1600]\ttraining's rmse: 0.0871643\ttraining's l2: 0.00759761\tvalid_1's rmse: 0.0916938\tvalid_1's l2: 0.00840775\n",
      "[1700]\ttraining's rmse: 0.085961\ttraining's l2: 0.00738929\tvalid_1's rmse: 0.090656\tvalid_1's l2: 0.00821852\n",
      "[1800]\ttraining's rmse: 0.084951\ttraining's l2: 0.00721667\tvalid_1's rmse: 0.0899005\tvalid_1's l2: 0.0080821\n",
      "[1900]\ttraining's rmse: 0.0839068\ttraining's l2: 0.00704035\tvalid_1's rmse: 0.089068\tvalid_1's l2: 0.0079331\n",
      "[2000]\ttraining's rmse: 0.0829699\ttraining's l2: 0.00688401\tvalid_1's rmse: 0.0883546\tvalid_1's l2: 0.00780654\n",
      "[2100]\ttraining's rmse: 0.0821032\ttraining's l2: 0.00674093\tvalid_1's rmse: 0.0876859\tvalid_1's l2: 0.00768881\n",
      "[2200]\ttraining's rmse: 0.081256\ttraining's l2: 0.00660253\tvalid_1's rmse: 0.0870214\tvalid_1's l2: 0.00757273\n",
      "[2300]\ttraining's rmse: 0.0804379\ttraining's l2: 0.00647025\tvalid_1's rmse: 0.0863823\tvalid_1's l2: 0.00746191\n",
      "[2400]\ttraining's rmse: 0.0797038\ttraining's l2: 0.00635269\tvalid_1's rmse: 0.0858305\tvalid_1's l2: 0.00736687\n",
      "[2500]\ttraining's rmse: 0.0789468\ttraining's l2: 0.00623259\tvalid_1's rmse: 0.0852703\tvalid_1's l2: 0.00727102\n",
      "[2600]\ttraining's rmse: 0.0783008\ttraining's l2: 0.00613101\tvalid_1's rmse: 0.0848087\tvalid_1's l2: 0.00719252\n",
      "[2700]\ttraining's rmse: 0.0776513\ttraining's l2: 0.00602973\tvalid_1's rmse: 0.0843197\tvalid_1's l2: 0.00710981\n",
      "[2800]\ttraining's rmse: 0.0770777\ttraining's l2: 0.00594097\tvalid_1's rmse: 0.0839325\tvalid_1's l2: 0.00704466\n",
      "[2900]\ttraining's rmse: 0.0764963\ttraining's l2: 0.00585169\tvalid_1's rmse: 0.0835455\tvalid_1's l2: 0.00697984\n",
      "[3000]\ttraining's rmse: 0.075959\ttraining's l2: 0.00576977\tvalid_1's rmse: 0.0831876\tvalid_1's l2: 0.00692018\n",
      "[3100]\ttraining's rmse: 0.0754557\ttraining's l2: 0.00569356\tvalid_1's rmse: 0.0828618\tvalid_1's l2: 0.00686608\n",
      "[3200]\ttraining's rmse: 0.0749149\ttraining's l2: 0.00561224\tvalid_1's rmse: 0.082492\tvalid_1's l2: 0.00680493\n",
      "[3300]\ttraining's rmse: 0.0743848\ttraining's l2: 0.00553309\tvalid_1's rmse: 0.0821277\tvalid_1's l2: 0.00674497\n",
      "[3400]\ttraining's rmse: 0.0738932\ttraining's l2: 0.0054602\tvalid_1's rmse: 0.081805\tvalid_1's l2: 0.00669206\n",
      "[3500]\ttraining's rmse: 0.0734353\ttraining's l2: 0.00539274\tvalid_1's rmse: 0.0815121\tvalid_1's l2: 0.00664422\n",
      "[3600]\ttraining's rmse: 0.0729693\ttraining's l2: 0.00532452\tvalid_1's rmse: 0.0812196\tvalid_1's l2: 0.00659662\n",
      "[3700]\ttraining's rmse: 0.0725394\ttraining's l2: 0.00526197\tvalid_1's rmse: 0.0809668\tvalid_1's l2: 0.00655563\n",
      "[3800]\ttraining's rmse: 0.072119\ttraining's l2: 0.00520115\tvalid_1's rmse: 0.0806896\tvalid_1's l2: 0.00651081\n",
      "[3900]\ttraining's rmse: 0.0717081\ttraining's l2: 0.00514206\tvalid_1's rmse: 0.0804265\tvalid_1's l2: 0.00646842\n",
      "[4000]\ttraining's rmse: 0.0712905\ttraining's l2: 0.00508234\tvalid_1's rmse: 0.0801718\tvalid_1's l2: 0.00642751\n",
      "[4100]\ttraining's rmse: 0.0709137\ttraining's l2: 0.00502876\tvalid_1's rmse: 0.0799595\tvalid_1's l2: 0.00639353\n",
      "[4200]\ttraining's rmse: 0.070503\ttraining's l2: 0.00497067\tvalid_1's rmse: 0.0797113\tvalid_1's l2: 0.0063539\n",
      "[4300]\ttraining's rmse: 0.0701303\ttraining's l2: 0.00491826\tvalid_1's rmse: 0.0794826\tvalid_1's l2: 0.00631748\n",
      "[4400]\ttraining's rmse: 0.0698031\ttraining's l2: 0.00487247\tvalid_1's rmse: 0.0793072\tvalid_1's l2: 0.00628963\n",
      "[4500]\ttraining's rmse: 0.0694423\ttraining's l2: 0.00482223\tvalid_1's rmse: 0.0791027\tvalid_1's l2: 0.00625724\n",
      "[4600]\ttraining's rmse: 0.0691378\ttraining's l2: 0.00478003\tvalid_1's rmse: 0.0789461\tvalid_1's l2: 0.00623249\n",
      "[4700]\ttraining's rmse: 0.0688014\ttraining's l2: 0.00473363\tvalid_1's rmse: 0.0787526\tvalid_1's l2: 0.00620196\n",
      "[4800]\ttraining's rmse: 0.0684667\ttraining's l2: 0.00468768\tvalid_1's rmse: 0.0785624\tvalid_1's l2: 0.00617205\n",
      "[4900]\ttraining's rmse: 0.0681341\ttraining's l2: 0.00464225\tvalid_1's rmse: 0.0783957\tvalid_1's l2: 0.00614588\n",
      "[5000]\ttraining's rmse: 0.0678252\ttraining's l2: 0.00460025\tvalid_1's rmse: 0.0782331\tvalid_1's l2: 0.00612042\n",
      "[5100]\ttraining's rmse: 0.0675338\ttraining's l2: 0.00456082\tvalid_1's rmse: 0.0780775\tvalid_1's l2: 0.0060961\n",
      "[5200]\ttraining's rmse: 0.0672256\ttraining's l2: 0.00451928\tvalid_1's rmse: 0.0779216\tvalid_1's l2: 0.00607178\n",
      "[5300]\ttraining's rmse: 0.0669501\ttraining's l2: 0.00448231\tvalid_1's rmse: 0.0777948\tvalid_1's l2: 0.00605204\n",
      "[5400]\ttraining's rmse: 0.0666774\ttraining's l2: 0.00444588\tvalid_1's rmse: 0.0776594\tvalid_1's l2: 0.00603098\n",
      "[5500]\ttraining's rmse: 0.0663777\ttraining's l2: 0.00440601\tvalid_1's rmse: 0.0775059\tvalid_1's l2: 0.00600716\n",
      "[5600]\ttraining's rmse: 0.0660992\ttraining's l2: 0.00436911\tvalid_1's rmse: 0.07737\tvalid_1's l2: 0.00598612\n",
      "[5700]\ttraining's rmse: 0.065838\ttraining's l2: 0.00433465\tvalid_1's rmse: 0.077248\tvalid_1's l2: 0.00596726\n",
      "[5800]\ttraining's rmse: 0.0655914\ttraining's l2: 0.00430223\tvalid_1's rmse: 0.0771438\tvalid_1's l2: 0.00595117\n",
      "[5900]\ttraining's rmse: 0.0653068\ttraining's l2: 0.00426498\tvalid_1's rmse: 0.0770074\tvalid_1's l2: 0.00593014\n",
      "[6000]\ttraining's rmse: 0.0650729\ttraining's l2: 0.00423448\tvalid_1's rmse: 0.0769044\tvalid_1's l2: 0.00591429\n",
      "[6100]\ttraining's rmse: 0.0648245\ttraining's l2: 0.00420222\tvalid_1's rmse: 0.0767841\tvalid_1's l2: 0.00589579\n",
      "[6200]\ttraining's rmse: 0.0645977\ttraining's l2: 0.00417286\tvalid_1's rmse: 0.0766766\tvalid_1's l2: 0.00587931\n",
      "[6300]\ttraining's rmse: 0.0643574\ttraining's l2: 0.00414187\tvalid_1's rmse: 0.0765684\tvalid_1's l2: 0.00586272\n",
      "[6400]\ttraining's rmse: 0.0641358\ttraining's l2: 0.00411341\tvalid_1's rmse: 0.076459\tvalid_1's l2: 0.00584598\n",
      "[6500]\ttraining's rmse: 0.0638991\ttraining's l2: 0.00408309\tvalid_1's rmse: 0.0763507\tvalid_1's l2: 0.00582943\n",
      "[6600]\ttraining's rmse: 0.0636766\ttraining's l2: 0.00405472\tvalid_1's rmse: 0.0762637\tvalid_1's l2: 0.00581616\n",
      "[6700]\ttraining's rmse: 0.0634495\ttraining's l2: 0.00402584\tvalid_1's rmse: 0.0761766\tvalid_1's l2: 0.00580287\n",
      "[6800]\ttraining's rmse: 0.0632415\ttraining's l2: 0.00399949\tvalid_1's rmse: 0.0760981\tvalid_1's l2: 0.00579093\n",
      "[6900]\ttraining's rmse: 0.0630182\ttraining's l2: 0.0039713\tvalid_1's rmse: 0.0759959\tvalid_1's l2: 0.00577538\n",
      "[7000]\ttraining's rmse: 0.0628115\ttraining's l2: 0.00394529\tvalid_1's rmse: 0.0759199\tvalid_1's l2: 0.00576382\n",
      "[7100]\ttraining's rmse: 0.062618\ttraining's l2: 0.00392101\tvalid_1's rmse: 0.0758452\tvalid_1's l2: 0.00575249\n",
      "[7200]\ttraining's rmse: 0.0624328\ttraining's l2: 0.00389786\tvalid_1's rmse: 0.0757692\tvalid_1's l2: 0.00574097\n",
      "[7300]\ttraining's rmse: 0.0622369\ttraining's l2: 0.00387343\tvalid_1's rmse: 0.0756969\tvalid_1's l2: 0.00573003\n",
      "[7400]\ttraining's rmse: 0.0620216\ttraining's l2: 0.00384668\tvalid_1's rmse: 0.0756163\tvalid_1's l2: 0.00571783\n",
      "[7500]\ttraining's rmse: 0.0618311\ttraining's l2: 0.00382309\tvalid_1's rmse: 0.0755439\tvalid_1's l2: 0.00570688\n",
      "[7600]\ttraining's rmse: 0.061629\ttraining's l2: 0.00379814\tvalid_1's rmse: 0.0754728\tvalid_1's l2: 0.00569614\n",
      "[7700]\ttraining's rmse: 0.0614534\ttraining's l2: 0.00377652\tvalid_1's rmse: 0.0754028\tvalid_1's l2: 0.00568558\n",
      "[7800]\ttraining's rmse: 0.0612584\ttraining's l2: 0.00375259\tvalid_1's rmse: 0.0753214\tvalid_1's l2: 0.00567331\n",
      "[7900]\ttraining's rmse: 0.061071\ttraining's l2: 0.00372967\tvalid_1's rmse: 0.0752528\tvalid_1's l2: 0.00566299\n",
      "[8000]\ttraining's rmse: 0.0608586\ttraining's l2: 0.00370377\tvalid_1's rmse: 0.0751847\tvalid_1's l2: 0.00565275\n",
      "[8100]\ttraining's rmse: 0.0606774\ttraining's l2: 0.00368174\tvalid_1's rmse: 0.0751146\tvalid_1's l2: 0.0056422\n",
      "[8200]\ttraining's rmse: 0.0604931\ttraining's l2: 0.00365942\tvalid_1's rmse: 0.0750531\tvalid_1's l2: 0.00563296\n",
      "[8300]\ttraining's rmse: 0.0603188\ttraining's l2: 0.00363836\tvalid_1's rmse: 0.0749898\tvalid_1's l2: 0.00562348\n",
      "[8400]\ttraining's rmse: 0.0601518\ttraining's l2: 0.00361823\tvalid_1's rmse: 0.0749362\tvalid_1's l2: 0.00561543\n",
      "[8500]\ttraining's rmse: 0.0599738\ttraining's l2: 0.00359685\tvalid_1's rmse: 0.0748856\tvalid_1's l2: 0.00560785\n",
      "[8600]\ttraining's rmse: 0.0598042\ttraining's l2: 0.00357654\tvalid_1's rmse: 0.074839\tvalid_1's l2: 0.00560087\n",
      "[8700]\ttraining's rmse: 0.0596335\ttraining's l2: 0.00355616\tvalid_1's rmse: 0.0747894\tvalid_1's l2: 0.00559346\n",
      "[8800]\ttraining's rmse: 0.0594738\ttraining's l2: 0.00353714\tvalid_1's rmse: 0.0747378\tvalid_1's l2: 0.00558574\n",
      "[8900]\ttraining's rmse: 0.0593166\ttraining's l2: 0.00351846\tvalid_1's rmse: 0.0746929\tvalid_1's l2: 0.00557902\n",
      "[9000]\ttraining's rmse: 0.0591685\ttraining's l2: 0.00350092\tvalid_1's rmse: 0.0746544\tvalid_1's l2: 0.00557328\n",
      "[9100]\ttraining's rmse: 0.0590044\ttraining's l2: 0.00348152\tvalid_1's rmse: 0.0746038\tvalid_1's l2: 0.00556572\n",
      "[9200]\ttraining's rmse: 0.0588437\ttraining's l2: 0.00346258\tvalid_1's rmse: 0.0745571\tvalid_1's l2: 0.00555877\n",
      "[9300]\ttraining's rmse: 0.0586974\ttraining's l2: 0.00344539\tvalid_1's rmse: 0.0745099\tvalid_1's l2: 0.00555173\n",
      "[9400]\ttraining's rmse: 0.0585425\ttraining's l2: 0.00342722\tvalid_1's rmse: 0.0744686\tvalid_1's l2: 0.00554558\n",
      "[9500]\ttraining's rmse: 0.0583972\ttraining's l2: 0.00341024\tvalid_1's rmse: 0.0744241\tvalid_1's l2: 0.00553895\n",
      "[9600]\ttraining's rmse: 0.0582492\ttraining's l2: 0.00339297\tvalid_1's rmse: 0.0743803\tvalid_1's l2: 0.00553243\n",
      "[9700]\ttraining's rmse: 0.0581056\ttraining's l2: 0.00337626\tvalid_1's rmse: 0.0743358\tvalid_1's l2: 0.00552581\n",
      "[9800]\ttraining's rmse: 0.0579577\ttraining's l2: 0.0033591\tvalid_1's rmse: 0.0742871\tvalid_1's l2: 0.00551857\n",
      "[9900]\ttraining's rmse: 0.0577954\ttraining's l2: 0.00334031\tvalid_1's rmse: 0.0742397\tvalid_1's l2: 0.00551154\n",
      "[10000]\ttraining's rmse: 0.0576597\ttraining's l2: 0.00332464\tvalid_1's rmse: 0.0742017\tvalid_1's l2: 0.00550589\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.0576597\ttraining's l2: 0.00332464\tvalid_1's rmse: 0.0742017\tvalid_1's l2: 0.00550589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.8, n_estimators=10000, objective=&#x27;regression&#x27;,\n",
       "              random_state=42, subsample=0.8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.8, n_estimators=10000, objective=&#x27;regression&#x27;,\n",
       "              random_state=42, subsample=0.8)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.8, n_estimators=10000, objective='regression',\n",
       "              random_state=42, subsample=0.8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train, y_train, # 학습 데이터를 입력합니다.\n",
    "    eval_set=[(x_train, y_train), (x_test, y_test)], # 평가셋을 지정합니다.\n",
    "    eval_metric ='rmse', # 평가과정에서 사용할 평가함수를 지정합니다.\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(period=100, show_stdv=True)], # 앞서 지정했던 callback함수와 동일하게 지정합니다.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 학습된 모델을 저장합니다. Pickle 라이브러리를 이용하겠습니다.\n",
    "with open('model/saved_model_02.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test)\n",
    "test_pred = np.expm1(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(test_pred.astype(int), columns=[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9272 entries, 0 to 9271\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   target  9272 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 72.6 KB\n"
     ]
    }
   ],
   "source": [
    "preds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv('../data/sub/output_lightgbm_001.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
